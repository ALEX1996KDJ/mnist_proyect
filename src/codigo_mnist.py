# ğŸ“¦ ImportaciÃ³n de librerÃ­as
import tensorflow as tf
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import os

# ğŸ“‚ Crear carpetas para guardar resultados
os.makedirs("modelo_guardado", exist_ok=True)
os.makedirs("graficas", exist_ok=True)

# ğŸ“¥ Carga de datos MNIST
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# ğŸ”§ NormalizaciÃ³n
x_train = x_train / 255.0
x_test = x_test / 255.0

# ğŸ§  DefiniciÃ³n del modelo
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dropout(0.25),
    Dense(64, activation='relu'),
    Dropout(0.25),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # 10 clases porque MNIST va del 0 al 9
])

# âš™ï¸ CompilaciÃ³n del modelo
model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',  # porque las etiquetas son enteros (no one-hot)
    metrics=['accuracy']
)

# ğŸ‹ï¸ Entrenamiento del modelo
history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(x_test, y_test),
    verbose=1
)

# ğŸ’¾ Guardar el modelo entrenado
model.save("modelo_guardado/mi_modelo.h5")

# ğŸ“ˆ GrÃ¡ficas de entrenamiento
plt.figure(figsize=(12, 5))

# PrecisiÃ³n
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='PrecisiÃ³n entrenamiento')
plt.plot(history.history['val_accuracy'], label='PrecisiÃ³n validaciÃ³n')
plt.title('PrecisiÃ³n del modelo')
plt.xlabel('Ã‰poca')
plt.ylabel('PrecisiÃ³n')
plt.legend()

# PÃ©rdida
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='PÃ©rdida entrenamiento')
plt.plot(history.history['val_loss'], label='PÃ©rdida validaciÃ³n')
plt.title('PÃ©rdida del modelo')
plt.xlabel('Ã‰poca')
plt.ylabel('PÃ©rdida')
plt.legend()

# Guardar imagen
plt.savefig("graficas/entrenamiento.png")
plt.show()
